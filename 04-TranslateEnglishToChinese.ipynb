{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d582fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from src.model import NeuralTranslationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed947456",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2360708",
   "metadata": {},
   "source": [
    "Load `NeuralTranslationModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ec0143",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_in_chinese_tokenized=46\n",
    "max_word_index=3438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69ef9914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_in shape: (1, 46, 128)\n",
      "decoder_in shape: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "translation_model = NeuralTranslationModel(encoder_input_shape=(max_len_in_chinese_tokenized,128),\n",
    "                                           decoder_input_shape=(max_word_index + 1, 128))\n",
    "\n",
    "# build the model by calling it\n",
    "encoder_in=tf.zeros([1,max_len_in_chinese_tokenized,128])\n",
    "decoder_in=tf.Variable([[1]])\n",
    "print(f'encoder_in shape: {encoder_in.shape}')\n",
    "print(f'decoder_in shape: {decoder_in.shape}')\n",
    "_ = translation_model((encoder_in,decoder_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52156986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f89157af0d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_model.load_weights('models/eng-to-ch/checkpoint_best/checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416803ee",
   "metadata": {},
   "source": [
    "Load pre-trained English embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e595ede1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = load_model('models/tf2-preview_nnlm-en-dim128_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88889587",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f896b",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbf082da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>chinese</th>\n",
       "      <th>english_split</th>\n",
       "      <th>chinese_split</th>\n",
       "      <th>chinese_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi .</td>\n",
       "      <td>嗨 。</td>\n",
       "      <td>[Hi, .]</td>\n",
       "      <td>[&lt;start&gt;, 嗨, 。, &lt;end&gt;]</td>\n",
       "      <td>[1, 1924, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi .</td>\n",
       "      <td>你好 。</td>\n",
       "      <td>[Hi, .]</td>\n",
       "      <td>[&lt;start&gt;, 你, 好, 。, &lt;end&gt;]</td>\n",
       "      <td>[1, 7, 33, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run .</td>\n",
       "      <td>你用跑的 。</td>\n",
       "      <td>[Run, .]</td>\n",
       "      <td>[&lt;start&gt;, 你, 用, 跑, 的, 。, &lt;end&gt;]</td>\n",
       "      <td>[1, 7, 95, 397, 5, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wait !</td>\n",
       "      <td>等等 ！</td>\n",
       "      <td>[Wait, !]</td>\n",
       "      <td>[&lt;start&gt;, 等, 等, ！, &lt;end&gt;]</td>\n",
       "      <td>[1, 208, 208, 160, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wait !</td>\n",
       "      <td>等一下 ！</td>\n",
       "      <td>[Wait, !]</td>\n",
       "      <td>[&lt;start&gt;, 等, 一, 下, ！, &lt;end&gt;]</td>\n",
       "      <td>[1, 208, 12, 46, 160, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   english  chinese english_split                    chinese_split  \\\n",
       "0    Hi .      嗨 。        [Hi, .]           [<start>, 嗨, 。, <end>]   \n",
       "1    Hi .     你好 。        [Hi, .]        [<start>, 你, 好, 。, <end>]   \n",
       "2   Run .   你用跑的 。       [Run, .]  [<start>, 你, 用, 跑, 的, 。, <end>]   \n",
       "3  Wait !     等等 ！      [Wait, !]        [<start>, 等, 等, ！, <end>]   \n",
       "4  Wait !    等一下 ！      [Wait, !]     [<start>, 等, 一, 下, ！, <end>]   \n",
       "\n",
       "          chinese_tokenized  \n",
       "0           [1, 1924, 3, 2]  \n",
       "1          [1, 7, 33, 3, 2]  \n",
       "2  [1, 7, 95, 397, 5, 3, 2]  \n",
       "3     [1, 208, 208, 160, 2]  \n",
       "4  [1, 208, 12, 46, 160, 2]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('data/cmn-processed-tokenized.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da622c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese = df['chinese']\n",
    "english = df['english']\n",
    "english_split = df['english_split']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e4ea4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe3096",
   "metadata": {},
   "source": [
    "### Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e53ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=[]\n",
    "with open('data/tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "\n",
    "tokenizer_config = tokenizer.get_config()\n",
    "word_index = json.loads(tokenizer_config['word_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe685737",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = word_index['<start>']\n",
    "end_token = word_index['<end>']\n",
    "inv_chinese_index = {value:key for key,value in tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26177d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef29a5a",
   "metadata": {},
   "source": [
    "### Making translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "307e6ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(english_split_in):\n",
    "    eng_embedding = embedding_layer(english_split_in)\n",
    "    eng_padded = tf.pad(eng_embedding, \n",
    "                        [[max_len_in_chinese_tokenized-len(eng_embedding), 0], \n",
    "                         [0, 0]], \n",
    "                        constant_values = 0)\n",
    "    english_expand = tf.expand_dims(eng_padded, 0)\n",
    "    hidden_state, cell_state = translation_model.encoder(english_expand)\n",
    "\n",
    "    current_translation = []\n",
    "    current_token = tf.Variable([[start_token]])\n",
    "\n",
    "    while (len(current_translation) <= max_len_in_chinese_tokenized):\n",
    "        out1, hidden_state, cell_state = translation_model.decoder(current_token,hidden_state,cell_state)\n",
    "        out2 = tf.argmax(out1, axis=2).numpy()[0,0]\n",
    "        current_token = tf.Variable([[out2]])\n",
    "        if out2 == end_token:\n",
    "            break\n",
    "        else:\n",
    "            current_translation.append(out2)\n",
    "    inv_tokenized = [inv_chinese_index[w] for w in current_translation]\n",
    "    inv_tokenized_string = ' '.join(inv_tokenized)\n",
    "    return inv_tokenized_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11fcc514",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_count = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4736db19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9823, 21897, 15761,  5972,  7261,  6796,  9512, 18266, 10582,\n",
       "        5166])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_indices = np.random.choice(english_split.index,full_test_count)\n",
    "test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0ecabaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_translations=[]\n",
    "for idx in test_indices:\n",
    "    tmp_result = translate(english_split[idx])\n",
    "    full_translations.append(tmp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf7f8878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English-Dataset</th>\n",
       "      <th>Chinese-Dataset</th>\n",
       "      <th>Chinese-Translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He is still very much alive .</td>\n",
       "      <td>他依旧充满活力 。</td>\n",
       "      <td>他 一 直 非 常 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He kept his promise and helped his brothers .</td>\n",
       "      <td>他履行了他的承诺 ， 并且帮助了他的兄弟 。</td>\n",
       "      <td>他 把 他 的 父 親 的 幫 助 他 們 不 想 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is there a post office near here ?</td>\n",
       "      <td>這附近有郵局嗎 ？</td>\n",
       "      <td>有 附 近 有 房 间 吗 ？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wait for me in the car .</td>\n",
       "      <td>在车里等一下 。</td>\n",
       "      <td>在 我 站 要 一 起 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't like eating meat .</td>\n",
       "      <td>我不喜歡吃肉了 。</td>\n",
       "      <td>我 不 喜 歡 吃 食 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tom looked at his notes .</td>\n",
       "      <td>汤姆看了看笔记 。</td>\n",
       "      <td>汤 姆 看 了 我 的 房 子 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tom passed away last night .</td>\n",
       "      <td>汤姆在昨晚去世了 。</td>\n",
       "      <td>汤 姆 昨 天 早 上 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What would you think if I did that ?</td>\n",
       "      <td>如果我那么做你会怎么想 ？</td>\n",
       "      <td>你 怎 麼 想 我 想 做 什 麼 ？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We're all praying for Japan .</td>\n",
       "      <td>我們全體為日本祈禱 。</td>\n",
       "      <td>我 們 在 一 個 棒 間 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This is too expensive !</td>\n",
       "      <td>这太贵了 。</td>\n",
       "      <td>这 是 真 的 。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  English-Dataset          Chinese-Dataset  \\\n",
       "0                  He is still very much alive .                他依旧充满活力 。    \n",
       "1  He kept his promise and helped his brothers .   他履行了他的承诺 ， 并且帮助了他的兄弟 。    \n",
       "2             Is there a post office near here ?                這附近有郵局嗎 ？    \n",
       "3                       Wait for me in the car .                 在车里等一下 。    \n",
       "4                     I don't like eating meat .                我不喜歡吃肉了 。    \n",
       "5                      Tom looked at his notes .                汤姆看了看笔记 。    \n",
       "6                   Tom passed away last night .               汤姆在昨晚去世了 。    \n",
       "7           What would you think if I did that ?            如果我那么做你会怎么想 ？    \n",
       "8                  We're all praying for Japan .              我們全體為日本祈禱 。    \n",
       "9                        This is too expensive !                   这太贵了 。    \n",
       "\n",
       "            Chinese-Translated  \n",
       "0                  他 一 直 非 常 。  \n",
       "1  他 把 他 的 父 親 的 幫 助 他 們 不 想 。  \n",
       "2              有 附 近 有 房 间 吗 ？  \n",
       "3                在 我 站 要 一 起 。  \n",
       "4                我 不 喜 歡 吃 食 。  \n",
       "5            汤 姆 看 了 我 的 房 子 。  \n",
       "6                汤 姆 昨 天 早 上 。  \n",
       "7          你 怎 麼 想 我 想 做 什 麼 ？  \n",
       "8              我 們 在 一 個 棒 間 。  \n",
       "9                    这 是 真 的 。  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_results = pd.DataFrame(data={'English-Dataset':english.loc[test_indices].reset_index(drop=True), \n",
    "                                     'Chinese-Dataset':chinese.loc[test_indices].reset_index(drop=True),\n",
    "                                     'Chinese-Translated':full_translations}, \n",
    "                               index=range(len(test_indices)))\n",
    "df_full_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
