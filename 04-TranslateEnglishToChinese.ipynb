{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12df9141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "GPU name: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print('GPU name: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce5c598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 12 15:40:44 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 165...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| N/A   40C    P0    13W /  N/A |    277MiB /  3903MiB |      3%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       880      G   /usr/lib/xorg/Xorg                 14MiB |\r\n",
      "|    0   N/A  N/A      1592      G   /usr/lib/xorg/Xorg                 51MiB |\r\n",
      "|    0   N/A  N/A     12048      C   ...da/envs/tf-gpu/bin/python      207MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38198fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1f0e3",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ae0958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = load_model('models/tf2-preview_nnlm-en-dim128_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ba7f790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "encoder=load_model('models/eng-to-chn/encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578b74d",
   "metadata": {},
   "source": [
    "Additional steps are required for loading the decoder model, due to the fact that it's a subclass `Model` object.\n",
    "\n",
    "Loaded model cannot be consumed directly. As a workaround, we need to redefine the `Decoder` class in this notebook, and load the saved weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32113857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = Embedding(input_dim = max_word_index + 1,output_dim =128,mask_zero = True)\n",
    "        self.lstm = LSTM(units=512, return_sequences=True, return_state=True)\n",
    "        self.dense = Dense(units=max_word_index + 1)\n",
    "\n",
    "    def call(self,inputs,hidden_state = None,cell_state = None):\n",
    "        h = self.embedding(inputs)\n",
    "        if hidden_state != None and cell_state != None:\n",
    "            lstm,hidden,cell = self.lstm(h,initial_state =[hidden_state,cell_state])\n",
    "        else:\n",
    "            lstm,hidden,cell = self.lstm(h)\n",
    "        h = self.dense(lstm)\n",
    "        return h,hidden,cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8769851c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "decoder=load_model('models/eng-to-chn/decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13e9256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.save_weights('models/eng-to-chn/decoder-weights/decoder-weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f9a83",
   "metadata": {},
   "source": [
    "Load tokenizer to get the `max_word_index` required to generate the decoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "644a4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=[]\n",
    "with open('data/tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "\n",
    "tokenizer_config = tokenizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe4fee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max word_index: 3438\n"
     ]
    }
   ],
   "source": [
    "word_index = json.loads(tokenizer_config['word_index'])\n",
    "max_word_index = max(word_index.values())\n",
    "print(f'Max word_index: {max_word_index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19ab3b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd781c21bb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder=Decoder()\n",
    "decoder.load_weights('models/eng-to-chn/decoder-weights/decoder-weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a54dcc5",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e692bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data/cmn-processed-tokenized.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42522f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>chinese</th>\n",
       "      <th>english_split</th>\n",
       "      <th>chinese_split</th>\n",
       "      <th>chinese_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi .</td>\n",
       "      <td>嗨 。</td>\n",
       "      <td>[Hi, .]</td>\n",
       "      <td>[&lt;start&gt;, 嗨, 。, &lt;end&gt;]</td>\n",
       "      <td>[1, 1924, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi .</td>\n",
       "      <td>你好 。</td>\n",
       "      <td>[Hi, .]</td>\n",
       "      <td>[&lt;start&gt;, 你, 好, 。, &lt;end&gt;]</td>\n",
       "      <td>[1, 7, 33, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run .</td>\n",
       "      <td>你用跑的 。</td>\n",
       "      <td>[Run, .]</td>\n",
       "      <td>[&lt;start&gt;, 你, 用, 跑, 的, 。, &lt;end&gt;]</td>\n",
       "      <td>[1, 7, 95, 397, 5, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wait !</td>\n",
       "      <td>等等 ！</td>\n",
       "      <td>[Wait, !]</td>\n",
       "      <td>[&lt;start&gt;, 等, 等, ！, &lt;end&gt;]</td>\n",
       "      <td>[1, 208, 208, 160, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wait !</td>\n",
       "      <td>等一下 ！</td>\n",
       "      <td>[Wait, !]</td>\n",
       "      <td>[&lt;start&gt;, 等, 一, 下, ！, &lt;end&gt;]</td>\n",
       "      <td>[1, 208, 12, 46, 160, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   english  chinese english_split                    chinese_split  \\\n",
       "0    Hi .      嗨 。        [Hi, .]           [<start>, 嗨, 。, <end>]   \n",
       "1    Hi .     你好 。        [Hi, .]        [<start>, 你, 好, 。, <end>]   \n",
       "2   Run .   你用跑的 。       [Run, .]  [<start>, 你, 用, 跑, 的, 。, <end>]   \n",
       "3  Wait !     等等 ！      [Wait, !]        [<start>, 等, 等, ！, <end>]   \n",
       "4  Wait !    等一下 ！      [Wait, !]     [<start>, 等, 一, 下, ！, <end>]   \n",
       "\n",
       "          chinese_tokenized  \n",
       "0           [1, 1924, 3, 2]  \n",
       "1          [1, 7, 33, 3, 2]  \n",
       "2  [1, 7, 95, 397, 5, 3, 2]  \n",
       "3     [1, 208, 208, 160, 2]  \n",
       "4  [1, 208, 12, 46, 160, 2]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef93bbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length in tokenized chinese sequence: 46\n"
     ]
    }
   ],
   "source": [
    "tokenizer_seq = df['chinese_tokenized']\n",
    "max_len_in_chinese_tokenized = max([len(item) for item in tokenizer_seq])\n",
    "\n",
    "print(f'Max length in tokenized chinese sequence: {max_len_in_chinese_tokenized}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1b95153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Hi . \n",
       "1      Hi . \n",
       "2     Run . \n",
       "3    Wait ! \n",
       "4    Wait ! \n",
       "Name: english, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english = df['english']\n",
    "chinese = df['chinese']\n",
    "english_split = df['english_split']\n",
    "english.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af428121",
   "metadata": {},
   "source": [
    "### Translation function development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276ed055",
   "metadata": {},
   "source": [
    "Develop the translation function with a sample English sentence at index `500`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d22be46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled English sentence: \"['Whose', 'is', 'it', '?']\"\n"
     ]
    }
   ],
   "source": [
    "test_index = 500\n",
    "print(f'Sampled English sentence: \"{english_split[test_index]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a37e74",
   "metadata": {},
   "source": [
    "Embed the input English sentence into the pre-trained 128 length embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12385322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: 4\n",
      "Embedding layer output shape: (4, 128)\n"
     ]
    }
   ],
   "source": [
    "eng_embedding = embedding_layer(english_split[test_index])\n",
    "print(f'Input shape: {len(english_split[test_index])}')\n",
    "print(f'Embedding layer output shape: {eng_embedding.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3035d2",
   "metadata": {},
   "source": [
    "Apply `prior` zero padding to the embedding to match the encoder model trained, which expect `46` English words input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4482558",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_padded = tf.pad(eng_embedding, \n",
    "                    [[max_len_in_chinese_tokenized-len(eng_embedding), 0], \n",
    "                     [0, 0]], \n",
    "                    constant_values = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bd60358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding layer output shape: (46, 128)\n"
     ]
    }
   ],
   "source": [
    "print(f'Padding layer output shape: {eng_padded.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fb1b4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 4 elements of 1-th embedding: [0. 0. 0. 0.]\n",
      "First 4 elements of 2-th embedding: [0. 0. 0. 0.]\n",
      "First 4 elements of 3-th embedding: [0. 0. 0. 0.]\n",
      "First 4 elements of 4-th embedding: [0. 0. 0. 0.]\n",
      "First 4 elements of 5-th embedding: [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'First 4 elements of {i+1}-th embedding: {eng_padded[i,0:4]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "732f378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 4 elements of 42-th embedding: [0. 0. 0. 0.]\n",
      "First 4 elements of 43-th embedding: [ 0.02321582 -0.00299811  0.01814764  0.12830451]\n",
      "First 4 elements of 44-th embedding: [ 0.22104432 -0.01606884  0.00432623  0.04148778]\n",
      "First 4 elements of 45-th embedding: [ 0.03716571 -0.02912278  0.12921344  0.06043958]\n",
      "First 4 elements of 46-th embedding: [-0.01335301  0.11507112  0.12568313  0.08377809]\n"
     ]
    }
   ],
   "source": [
    "end_index=max_len_in_chinese_tokenized\n",
    "for i in np.arange(end_index-5,end_index,1):\n",
    "    print(f'First 4 elements of {i+1}-th embedding: {eng_padded[i,0:4]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcc9224",
   "metadata": {},
   "source": [
    "Expand the dimension of the input at `axis=0` to represent the batch axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1844ffef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expand dim english shape: (1, 46, 128)\n"
     ]
    }
   ],
   "source": [
    "english_expand = tf.expand_dims(eng_padded, 0)\n",
    "print(f'Expand dim english shape: {english_expand.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fd8f6e",
   "metadata": {},
   "source": [
    "Feed the input into the encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4d192fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state, cell_state = encoder(english_expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8144105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden state shape: (1, 512)\n",
      "Encoder cell state shape: (1, 512)\n"
     ]
    }
   ],
   "source": [
    "print(f'Encoder hidden state shape: {hidden_state.shape}')\n",
    "print(f'Encoder cell state shape: {cell_state.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73cc0bc",
   "metadata": {},
   "source": [
    "Extract the `<start>` and `<end>` tokens' `word_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a475d656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> token word index: 1\n",
      "<end> token word index: 2\n"
     ]
    }
   ],
   "source": [
    "start_token = word_index['<start>']\n",
    "end_token = word_index['<end>']\n",
    "\n",
    "print(f'<start> token word index: {start_token}')\n",
    "print(f'<end> token word index: {end_token}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4784bd87",
   "metadata": {},
   "source": [
    "- A `<start>` token is passed in as the first input, which is embedded using a learned Chinese word embedding.\n",
    "- The decoder RNN then makes a prediction for the next Chinese word, which during inference is then passed in as the following input, and this process is repeated until the special `<end>` token is emitted from the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a613b4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_token shape: (1, 1)\n",
      "Translation: [7, 5, 11, 35, 48, 9]\n"
     ]
    }
   ],
   "source": [
    "current_translation = []\n",
    "current_token = tf.Variable([[start_token]])\n",
    "print(f'current_token shape: {current_token.shape}')\n",
    "\n",
    "while (len(current_translation) <= max_len_in_chinese_tokenized):\n",
    "    out1, hidden_state, cell_state = decoder(current_token,hidden_state,cell_state)\n",
    "    out2 = tf.argmax(out1, axis=2).numpy()[0,0]\n",
    "    current_token = tf.Variable([[out2]])\n",
    "    if out2 == end_token:\n",
    "        break\n",
    "    else:\n",
    "        current_translation.append(out2)\n",
    "\n",
    "print(f'Translation: {current_translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3756f65",
   "metadata": {},
   "source": [
    "Getting inverse token index dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8f99777",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_chinese_index = {value:key for key,value in tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b8545c",
   "metadata": {},
   "source": [
    "Mapping the tokenized translation output into Chinese characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8eeb3f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你', '的', '是', '什', '麼', '？']\n"
     ]
    }
   ],
   "source": [
    "inv_tokenized = [inv_chinese_index[w] for w in current_translation]\n",
    "print(f'{inv_tokenized}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3043805",
   "metadata": {},
   "source": [
    "Getting Chinese sentence string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55571dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你 的 是 什 麼 ？\n"
     ]
    }
   ],
   "source": [
    "print(f\"{' '.join(inv_tokenized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a36440b",
   "metadata": {},
   "source": [
    "### Generalize into a single `translate` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60eeea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(english_split_in):\n",
    "    eng_embedding = embedding_layer(english_split_in)\n",
    "    eng_padded = tf.pad(eng_embedding, \n",
    "                        [[max_len_in_chinese_tokenized-len(eng_embedding), 0], \n",
    "                         [0, 0]], \n",
    "                        constant_values = 0)\n",
    "    english_expand = tf.expand_dims(eng_padded, 0)\n",
    "    hidden_state, cell_state = encoder(english_expand)\n",
    "\n",
    "    current_translation = []\n",
    "    current_token = tf.Variable([[start_token]])\n",
    "\n",
    "    while (len(current_translation) <= max_len_in_chinese_tokenized):\n",
    "        out1, hidden_state, cell_state = decoder(current_token,hidden_state,cell_state)\n",
    "        out2 = tf.argmax(out1, axis=2).numpy()[0,0]\n",
    "        current_token = tf.Variable([[out2]])\n",
    "        if out2 == end_token:\n",
    "            break\n",
    "        else:\n",
    "            current_translation.append(out2)\n",
    "    inv_tokenized = [inv_chinese_index[w] for w in current_translation]\n",
    "    inv_tokenized_string = ' '.join(inv_tokenized)\n",
    "    return inv_tokenized_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e860361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'English': 'Whose is it ? ', 'Chinese': '你 的 是 什 麼 ？'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'English':english[test_index], \n",
    "                                 'Chinese':translate(english_split[test_index])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f893b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Chinese-Reference</th>\n",
       "      <th>Chinese-Translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Whose is it ?</td>\n",
       "      <td>这是谁的 ？</td>\n",
       "      <td>你 的 是 什 麼 ？</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          English Chinese-Reference Chinese-Translated\n",
       "0  Whose is it ?            这是谁的 ？         你 的 是 什 麼 ？"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_result = pd.DataFrame(data={'English':english[test_index], \n",
    "                                      'Chinese-Reference':chinese[test_index],\n",
    "                                      'Chinese-Translated':translate(english_split[test_index])}, \n",
    "                                index=[0])\n",
    "df_sample_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2c7bc",
   "metadata": {},
   "source": [
    "### Full translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43e1f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_count = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92bb7b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14777, 21788, 18257, 22888,   194,  3120,  3085,  9726, 20885,\n",
       "       18153])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_indices = np.random.choice(english_split.index,full_test_count)\n",
    "test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7525c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_translations=[]\n",
    "for idx in test_indices:\n",
    "    tmp_result = translate(english_split[idx])\n",
    "    full_translations.append(tmp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2af72717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Chinese-Reference</th>\n",
       "      <th>Chinese-Translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>She greeted him waving her hand .</td>\n",
       "      <td>她揮著手向他打招呼 。</td>\n",
       "      <td>她 把 他 的 手 脸 在 看 下 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom and Mary have a very good relationship .</td>\n",
       "      <td>汤姆和玛丽关系很好 。</td>\n",
       "      <td>汤 姆 和 玛 丽 一 个 很 好 的 想 忙 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What have you been doing this week ?</td>\n",
       "      <td>你這個星期一直在做什麼 ？</td>\n",
       "      <td>你 今 天 做 做 什 么 ？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is necessary for you to go there immediatel...</td>\n",
       "      <td>你必需马上去那儿 。</td>\n",
       "      <td>你 必 須 做 到 它 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lie still .</td>\n",
       "      <td>躺着不动 。</td>\n",
       "      <td>我 们 发 着 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This is impossible .</td>\n",
       "      <td>这不可能 。</td>\n",
       "      <td>这 是 不 要 的 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The lid won't open .</td>\n",
       "      <td>這蓋子打不開 。</td>\n",
       "      <td>它 不 開 了 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Could you take this ,  please ?</td>\n",
       "      <td>請你拿這個好嗎 ？</td>\n",
       "      <td>你 能 幫 你 嗎 ？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Have you ever seen the man in this photo ?</td>\n",
       "      <td>你见过这张照片上的男人吗 ？</td>\n",
       "      <td>你 看 见 看 看 你 的 照 照 照 嗎 ？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Thousands of people died of hunger .</td>\n",
       "      <td>數千人死於飢餓 。</td>\n",
       "      <td>人 們 有 一 個 人 都 死 的 。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English Chinese-Reference  \\\n",
       "0                 She greeted him waving her hand .       她揮著手向他打招呼 。    \n",
       "1      Tom and Mary have a very good relationship .       汤姆和玛丽关系很好 。    \n",
       "2              What have you been doing this week ?     你這個星期一直在做什麼 ？    \n",
       "3  It is necessary for you to go there immediatel...       你必需马上去那儿 。    \n",
       "4                                       Lie still .            躺着不动 。    \n",
       "5                              This is impossible .            这不可能 。    \n",
       "6                              The lid won't open .          這蓋子打不開 。    \n",
       "7                   Could you take this ,  please ?         請你拿這個好嗎 ？    \n",
       "8        Have you ever seen the man in this photo ?    你见过这张照片上的男人吗 ？    \n",
       "9              Thousands of people died of hunger .         數千人死於飢餓 。    \n",
       "\n",
       "          Chinese-Translated  \n",
       "0        她 把 他 的 手 脸 在 看 下 。  \n",
       "1  汤 姆 和 玛 丽 一 个 很 好 的 想 忙 。  \n",
       "2            你 今 天 做 做 什 么 ？  \n",
       "3              你 必 須 做 到 它 。  \n",
       "4                  我 们 发 着 。  \n",
       "5                这 是 不 要 的 。  \n",
       "6                  它 不 開 了 。  \n",
       "7                你 能 幫 你 嗎 ？  \n",
       "8    你 看 见 看 看 你 的 照 照 照 嗎 ？  \n",
       "9        人 們 有 一 個 人 都 死 的 。  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_results = pd.DataFrame(data={'English':english.loc[test_indices].reset_index(drop=True), \n",
    "                                     'Chinese-Reference':chinese.loc[test_indices].reset_index(drop=True),\n",
    "                                     'Chinese-Translated':full_translations}, \n",
    "                               index=range(len(test_indices)))\n",
    "df_full_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
