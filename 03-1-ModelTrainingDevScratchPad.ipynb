{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2472,
     "status": "ok",
     "timestamp": 1623203853441,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "-jZObsHeVlf3",
    "outputId": "627b0cee-6f61-4586-c3b9-c5e73ab184f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "GPU name: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "print('GPU name: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1623203853442,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "Qh2c_LWsaBMs",
    "outputId": "c414765b-2d56-48e7-f252-85ea2339503e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 15 16:58:24 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 165...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| N/A   45C    P0    14W /  N/A |    297MiB /  3903MiB |      8%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       883      G   /usr/lib/xorg/Xorg                 14MiB |\r\n",
      "|    0   N/A  N/A      1581      G   /usr/lib/xorg/Xorg                 71MiB |\r\n",
      "|    0   N/A  N/A     43827      C   ...da/envs/tf-gpu/bin/python      207MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1623203853445,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "a4cxfcIBWATp"
   },
   "outputs": [],
   "source": [
    "# folder path if running in Google Colab\n",
    "#googleDrivePathPrefix = 'drive/My Drive/Colab Notebooks'\n",
    "\n",
    "# folder path for running locally\n",
    "googleDrivePathPrefix = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1623203853847,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "5kF6pcsDWCST"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from math import ceil\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.layers import Layer,Input,Masking,LSTM,Embedding,Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1623203853848,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "eIAfULusWHvh",
    "outputId": "7a15641f-dcdd-4327-ed22-95e0a74b5d6b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>chinese</th>\n",
       "      <th>english_split</th>\n",
       "      <th>chinese_split</th>\n",
       "      <th>chinese_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi .</td>\n",
       "      <td>嗨 。</td>\n",
       "      <td>[Hi, .]</td>\n",
       "      <td>[&lt;start&gt;, 嗨, 。, &lt;end&gt;]</td>\n",
       "      <td>[1, 1924, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi .</td>\n",
       "      <td>你好 。</td>\n",
       "      <td>[Hi, .]</td>\n",
       "      <td>[&lt;start&gt;, 你, 好, 。, &lt;end&gt;]</td>\n",
       "      <td>[1, 7, 33, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run .</td>\n",
       "      <td>你用跑的 。</td>\n",
       "      <td>[Run, .]</td>\n",
       "      <td>[&lt;start&gt;, 你, 用, 跑, 的, 。, &lt;end&gt;]</td>\n",
       "      <td>[1, 7, 95, 397, 5, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wait !</td>\n",
       "      <td>等等 ！</td>\n",
       "      <td>[Wait, !]</td>\n",
       "      <td>[&lt;start&gt;, 等, 等, ！, &lt;end&gt;]</td>\n",
       "      <td>[1, 208, 208, 160, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wait !</td>\n",
       "      <td>等一下 ！</td>\n",
       "      <td>[Wait, !]</td>\n",
       "      <td>[&lt;start&gt;, 等, 一, 下, ！, &lt;end&gt;]</td>\n",
       "      <td>[1, 208, 12, 46, 160, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   english  chinese english_split                    chinese_split  \\\n",
       "0    Hi .      嗨 。        [Hi, .]           [<start>, 嗨, 。, <end>]   \n",
       "1    Hi .     你好 。        [Hi, .]        [<start>, 你, 好, 。, <end>]   \n",
       "2   Run .   你用跑的 。       [Run, .]  [<start>, 你, 用, 跑, 的, 。, <end>]   \n",
       "3  Wait !     等等 ！      [Wait, !]        [<start>, 等, 等, ！, <end>]   \n",
       "4  Wait !    等一下 ！      [Wait, !]     [<start>, 等, 一, 下, ！, <end>]   \n",
       "\n",
       "          chinese_tokenized  \n",
       "0           [1, 1924, 3, 2]  \n",
       "1          [1, 7, 33, 3, 2]  \n",
       "2  [1, 7, 95, 397, 5, 3, 2]  \n",
       "3     [1, 208, 208, 160, 2]  \n",
       "4  [1, 208, 12, 46, 160, 2]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(path.join(googleDrivePathPrefix,'data/cmn-processed-tokenized.json'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLnhqYM0WsiE"
   },
   "source": [
    "Apply padding to the tokenized german sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1623203853849,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "HhSNwEQrW-ju",
    "outputId": "6242bf0a-5ea3-4d79-d330-3d544c2a9049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length in tokenized chinese sequence: 46\n"
     ]
    }
   ],
   "source": [
    "tokenizer_seq = df['chinese_tokenized']\n",
    "max_len_in_chinese_tokenized = max([len(item) for item in tokenizer_seq])\n",
    "print(f'Max length in tokenized chinese sequence: {max_len_in_chinese_tokenized}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1623203854246,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "Q_UanUgEWtFK",
    "outputId": "55a4a37f-56ae-4f4a-e517-3d17e8722eb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded sequences: (24089, 46)\n"
     ]
    }
   ],
   "source": [
    "chinese_padded_seq = pad_sequences(tokenizer_seq,maxlen = None,padding = \"post\")\n",
    "print(f'Shape of padded sequences: {chinese_padded_seq.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1623203854247,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "2ce_bcLqXNzE",
    "outputId": "37842474-b5f2-4241-df6a-e5e0692f6023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th chinese sentence: ['<start>', '嗨', '。', '<end>']\n",
      "\n",
      "0-th tokenized sequence: [1, 1924, 3, 2]\n",
      "\n",
      "0-th padded sequence: [   1 1924    3    2    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"0-th chinese sentence: {df['chinese_split'].loc[0]}\\n\")\n",
    "print(f\"0-th tokenized sequence: {df['chinese_tokenized'].loc[0]}\\n\")\n",
    "print(f\"0-th padded sequence: {chinese_padded_seq[0]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eOKIarRY4Bs"
   },
   "source": [
    "Load a pre-trained english embedding layer from Tensorflow Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LyGf7_JZAnl"
   },
   "source": [
    "``` python\n",
    "# Load embedding module from Tensorflow Hub\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\",\n",
    "output_shape=[128], input_shape=[], dtype=tf.string)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iq4CtI64ZJhd"
   },
   "source": [
    "We will use the pre-downloaded model file instead, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1371,
     "status": "ok",
     "timestamp": 1623203855613,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "Qx-c4Nl8ZEnD",
    "outputId": "7c99ec35-2e0b-4828-9f83-7d05fcbfe5ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = load_model(path.join(googleDrivePathPrefix,'models/tf2-preview_nnlm-en-dim128_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1623203855618,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "MNnFB272YSdw",
    "outputId": "621f4582-cf17-4837-e409-30ff0b7fb8d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([7, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the layer\n",
    "embedding_layer(tf.constant([\"these\", \"aren't\", \"the\", \"droids\", \"you're\",\"looking\", \"for\"])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWWF_zUQZnMF"
   },
   "source": [
    "Train test split using `sklearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1623203855620,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "8Ot3F3T-ZihE"
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(df['english'].to_list(),chinese_padded_seq,train_size=0.8,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1623203855620,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "emxGsDWbZ2_b",
    "outputId": "0e4989b7-e50e-4f89-ac80-2ad345392899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train[0]:\n",
      "I know I owe you money . \n",
      "\n",
      "y_train[0]:\n",
      "[   1    4   49   47    4 1175    7  238    3    2    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'x_train[0]:\\n{x_train[0]}\\n')\n",
    "print(f'y_train[0]:\\n{y_train[0]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1623203855621,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "0fNj3GYsZ7Sy",
    "outputId": "6420fad7-986b-4628-d961-e741ca10f05c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train length: 19271\n",
      "\n",
      "y_train length: 19271\n",
      "\n",
      "x_test length: 4818\n",
      "\n",
      "y_test length: 4818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'x_train length: {len(x_train)}\\n')\n",
    "print(f'y_train length: {len(y_train)}\\n')\n",
    "print(f'x_test length: {len(x_test)}\\n')\n",
    "print(f'y_test length: {len(y_test)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIeWprwubKxa"
   },
   "source": [
    "Create `tf.data.Dataset` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1623203855622,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "yaTzWNBTauNK"
   },
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9b6moq4bugI"
   },
   "source": [
    "Mapping to split english text with spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1623203855622,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "bJxV8WhzbPI-"
   },
   "outputs": [],
   "source": [
    "def map_splitting(english,chinese):\n",
    "    return (tf.strings.split(english,' '),chinese)\n",
    "\n",
    "training_dataset_split=training_dataset.map(map_splitting)\n",
    "validation_dataset_split=validation_dataset.map(map_splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1623203855622,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "JGl4uHZnb2Mw",
    "outputId": "c6f5ae2d-6eba-47bd-a458-12ae26cec06f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None,), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(46,), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset_split.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7bSmHljcAbc"
   },
   "source": [
    "Inspect first element after map_splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1623203855623,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "z4HzHequb51e",
    "outputId": "f58db61e-a0ba-4cdc-926d-c1f44efe70c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train[0]:\n",
      "[b'I' b'know' b'I' b'owe' b'you' b'money' b'.' b'']\n",
      "\n",
      "x_train[0] shape:\n",
      "(8,)\n",
      "\n",
      "y_train[0]:\n",
      "[   1    4   49   47    4 1175    7  238    3    2    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "element = next(iter(training_dataset_split.take(1)))\n",
    "print(f'x_train[0]:\\n{element[0]}\\n')\n",
    "print(f'x_train[0] shape:\\n{element[0].shape}\\n')\n",
    "print(f'y_train[0]:\\n{element[1]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1yQhP8ycgLM"
   },
   "source": [
    "Filter dataset examples where the English sentence is more than 46."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1623203856192,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "hBPqJkVAcYQY"
   },
   "outputs": [],
   "source": [
    "# max_len_in_chinese_tokenized = 46\n",
    "\n",
    "def filter_less_eq_max_token_len(english,chinese):\n",
    "    return tf.less_equal(tf.shape(english)[0],tf.constant(max_len_in_chinese_tokenized))\n",
    "\n",
    "training_dataset_filter = training_dataset_split.filter(filter_less_eq_max_token_len)\n",
    "validation_dataset_filter = validation_dataset_split.filter(filter_less_eq_max_token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train = training_dataset_filter.reduce(0, lambda x, _: x + 1)\n",
    "count_val = validation_dataset_filter.reduce(0, lambda x, _: x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19271\n",
      "4818\n"
     ]
    }
   ],
   "source": [
    "print(f'{count_train}')\n",
    "print(f'{count_val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muwReJf-dLIx"
   },
   "source": [
    "Inspect 5 random samples from the filtered training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 429,
     "status": "ok",
     "timestamp": 1623203856617,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "h3v6RsUDdEwe",
    "outputId": "06a45ebf-967d-487c-8eb0-7b8fcf641a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th random sample shape:\n",
      "(7,)\n",
      "\n",
      "2-th random sample shape:\n",
      "(8,)\n",
      "\n",
      "3-th random sample shape:\n",
      "(11,)\n",
      "\n",
      "4-th random sample shape:\n",
      "(14,)\n",
      "\n",
      "5-th random sample shape:\n",
      "(8,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elements = iter(training_dataset_filter.shuffle(buffer_size=100).take(5))\n",
    "i=0\n",
    "for english,german in elements:\n",
    "    i+=1\n",
    "    print(f'{i}-th random sample shape:\\n{english.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aC76PTmmcDhg"
   },
   "source": [
    "Mapping to apply the pre-trained word embedding to english texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1623203855624,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "TlDLagIgb9An"
   },
   "outputs": [],
   "source": [
    "def map_embedding(english,chinese):\n",
    "    return (embedding_layer(english),chinese)\n",
    "\n",
    "training_dataset_embed=training_dataset_filter.map(map_embedding)\n",
    "validation_dataset_embed=validation_dataset_filter.map(map_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1623203856191,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "TMYBBvCtcPe3",
    "outputId": "249639ad-acde-4c6e-b13c-5816961f7892"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 128), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(46,), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset_embed.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytB8LUrXcVfv"
   },
   "source": [
    "Inspect first element after map_embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1623203856191,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "5EguuPj8cShZ",
    "outputId": "700bc328-ce88-454e-bb99-6307a816d5cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding mapping:\n",
      "\n",
      "x_train[0] shape:\n",
      "(8, 128)\n",
      "\n",
      "y_train[0]:\n",
      "[   1    4   49   47    4 1175    7  238    3    2    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "element = next(iter(training_dataset_embed.take(1)))\n",
    "print(f'Embedding mapping:\\n')\n",
    "print(f'x_train[0] shape:\\n{element[0].shape}\\n')\n",
    "print(f'y_train[0]:\\n{element[1]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DX3-AYyFdeTm"
   },
   "source": [
    "Padding english sentence embeddings to length of 46 (padding to prior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1623203856618,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "cExmRe2edRos"
   },
   "outputs": [],
   "source": [
    "def map_english_padding(english,chinese):\n",
    "    english_length = tf.shape(english)[0]\n",
    "    paddings = [[max_len_in_chinese_tokenized-english_length,0],\n",
    "                [0,0]\n",
    "               ]\n",
    "    return (tf.pad(english,paddings=paddings),chinese)\n",
    "\n",
    "training_dataset_english_padded=training_dataset_embed.map(map_english_padding)\n",
    "validation_dataset_english_padded=training_dataset_embed.map(map_english_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1623203856618,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "iC1wawtSeDwk",
    "outputId": "78710e93-f50f-408a-8088-c0528de2d612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train[0] shape:\n",
      "(46, 128)\n",
      "\n",
      "y_train[0]:\n",
      "[   1    4   49   47    4 1175    7  238    3    2    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "element = next(iter(training_dataset_english_padded.take(1)))\n",
    "print(f'x_train[0] shape:\\n{element[0].shape}\\n')\n",
    "print(f'y_train[0]:\\n{element[1]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1hADA85eRWG"
   },
   "source": [
    "Batching to batch_size of 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1623203856618,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "u51BOdNWeHsE"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "training_dataset_final = training_dataset_english_padded.\\\n",
    "                         repeat().\\\n",
    "                         batch(batch_size=batch_size)\n",
    "validation_dataset_final = validation_dataset_english_padded.\\\n",
    "                         repeat().\\\n",
    "                         batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7buz2MNNeXj9"
   },
   "source": [
    "Inspecting element_spec of final training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1623203856619,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "u8ZM7vHVeW2b",
    "outputId": "b7f80ee5-f8f0-4373-f3f0-44519f3a5bc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, None, 128), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None, 46), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset_final.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1623203856619,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "f1PtsTBjeZpj",
    "outputId": "d813c7d0-42cf-415e-f1cb-17cda728a40f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, None, 128), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None, 46), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset_final.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAPKDwT0efcF"
   },
   "source": [
    "Inspecting first batch of both training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1623203856620,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "Ps5VdL2jedIz",
    "outputId": "a5e31559-20bb-4f51-ae58-bfaa9ae53d06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train batch shape:\n",
      "(16, 46, 128)\n",
      "\n",
      "y_train batch shape:\n",
      "(16, 46)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "element = next(iter(training_dataset_final.take(1)))\n",
    "print(f'x_train batch shape:\\n{element[0].shape}\\n')\n",
    "print(f'y_train batch shape:\\n{element[1].shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1623203856934,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "n1-o6J0zeiMm",
    "outputId": "cddba212-06fe-4fd7-a75e-8edbd025a149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_validation batch shape:\n",
      "(16, 46, 128)\n",
      "\n",
      "y_validation batch shape:\n",
      "(16, 46)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "element = next(iter(validation_dataset_final.take(1)))\n",
    "print(f'x_validation batch shape:\\n{element[0].shape}\\n')\n",
    "print(f'y_validation batch shape:\\n{element[1].shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJTlqxqLf35S"
   },
   "source": [
    "### Create encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1623203856935,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "O8P3sqj6f3MF"
   },
   "outputs": [],
   "source": [
    "class EndTokenEmbedLayer(Layer):\n",
    "    def __init__(self):\n",
    "        super(EndTokenEmbedLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embedding_size = input_shape[-1]\n",
    "        self.embedding = self.add_weight(shape=(self.embedding_size,),\n",
    "                                         initializer='random_normal',\n",
    "                                         name='end_token_embedding')\n",
    "  \n",
    "    def call(self, inputs):\n",
    "        one_row = tf.reshape(self.embedding,(-1,1,self.embedding_size))\n",
    "        end_token_output = tf.tile(one_row,[tf.shape(inputs)[0],1,1])\n",
    "        return tf.concat((inputs,end_token_output),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1623203856936,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "nAyC8NpsfBLF",
    "outputId": "e59be782-a9cf-4319-ba77-73fdeac5e8c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentences shape: (16, 46, 128)\n",
      "\n",
      "English sentences (end token appended) shape: (16, 47, 128)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "endTokenlayer = EndTokenEmbedLayer()\n",
    "for english,german in iter(training_dataset_final.take(1)):\n",
    "    endTokenAdded = endTokenlayer(english)\n",
    "    print(f'English sentences shape: {english.shape}\\n')\n",
    "    print(f'English sentences (end token appended) shape: {endTokenAdded.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1623203856936,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "A4DBOEX7gkfr"
   },
   "outputs": [],
   "source": [
    "def Encoder(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "    h = EndTokenEmbedLayer()(inputs)\n",
    "    h = Masking(mask_value=0.)(h)\n",
    "    lstm , hidden_state, cell_state = LSTM(512,return_sequences=True,return_state=True)(h)\n",
    "    model = Model(inputs=inputs, outputs=[hidden_state, cell_state])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1330,
     "status": "ok",
     "timestamp": 1623203858260,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "rnjUPaqbgyrK",
    "outputId": "a983dec1-cc72-4a1a-acda-13c32bb197e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 46, 128)]         0         \n",
      "_________________________________________________________________\n",
      "end_token_embed_layer_1 (End (None, 47, 128)           128       \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, 47, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 47, 512), (None,  1312768   \n",
      "=================================================================\n",
      "Total params: 1,312,896\n",
      "Trainable params: 1,312,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# max_len_in_chinese_tokenized =46\n",
    "\n",
    "encoder = Encoder(input_shape=(max_len_in_chinese_tokenized,128))\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 705,
     "status": "ok",
     "timestamp": 1623203858962,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "yFo2riJTg0hB",
    "outputId": "a1eba4bb-3b0e-4654-e779-f830f2058304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_state shape: (16, 512)\n",
      "\n",
      "cell_state shape: (16, 512)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for english,chinese in iter(training_dataset_final.take(1)):\n",
    "    hidden_state, cell_state = encoder(english)\n",
    "    print(f'hidden_state shape: {hidden_state.shape}\\n')\n",
    "    print(f'cell_state shape: {cell_state.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOoVUvp5hWan"
   },
   "source": [
    "### Build the decoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1623203858963,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "88KvsnBmhQ3p"
   },
   "outputs": [],
   "source": [
    "tokenizer=[]\n",
    "with open(path.join(googleDrivePathPrefix,'data/tokenizer.json')) as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "\n",
    "tokenizer_config = tokenizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1623203858964,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "Wzbc2xEakJ9p",
    "outputId": "b3033f0c-66f8-455b-f419-9d9773ba9cfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3438"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = json.loads(tokenizer_config['word_index'])\n",
    "max_word_index = max(word_index.values())\n",
    "max_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1623203858964,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "h7l7VO70kY3t"
   },
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self,input_embedding_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = Embedding(input_dim = input_embedding_dim[0],\n",
    "                                   output_dim = input_embedding_dim[1],\n",
    "                                   mask_zero = True, trainable=True)\n",
    "        self.lstm = LSTM(units=512, return_sequences=True, return_state=True, trainable=True)\n",
    "        self.dense = Dense(units=max_word_index + 1, trainable=True)\n",
    "\n",
    "    def call(self,inputs,hidden_state = None,cell_state = None):\n",
    "        h = self.embedding(inputs)\n",
    "        if hidden_state != None and cell_state != None:\n",
    "            lstm,hidden,cell = self.lstm(h,initial_state =[hidden_state,cell_state])\n",
    "        else:\n",
    "            lstm,hidden,cell = self.lstm(h)\n",
    "        h = self.dense(lstm)\n",
    "        return h,hidden,cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_KM_aYRkv0B"
   },
   "source": [
    "Remark: Notice that input_dim is set to maximum word index + 1 because the embedding layer maps vocabulary word index into 0-indexed array. Thus the max word index, 3438 needs to be\n",
    "mapped into an array of 3439 elements, i.e. with the range of [0,3438]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1623203858965,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "egDt3HYWktCv"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(input_embedding_dim=(max_word_index + 1, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1144,
     "status": "ok",
     "timestamp": 1623203860100,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "-8dIVG14k-Ne",
    "outputId": "a3ecaf92-5d5e-4d0d-e663-141813a7b737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 46)\n",
      "lstm_out shape: (16, 46, 3439)\n",
      "\n",
      "hidden_state shape: (16, 512)\n",
      "\n",
      "cell_state shape: (16, 512)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for english,chinese in iter(training_dataset_final.take(1)):\n",
    "    print(chinese.shape)\n",
    "    lstm_out, hidden_state, cell_state = decoder(chinese)\n",
    "    print(f'lstm_out shape: {lstm_out.shape}\\n')\n",
    "    print(f'hidden_state shape: {hidden_state.shape}\\n')\n",
    "    print(f'cell_state shape: {cell_state.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1623203860101,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "JFkDbdHQlIKO",
    "outputId": "870945e9-13d7-4cd1-e225-c948c133dce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  440192    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  1312768   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  1764207   \n",
      "=================================================================\n",
      "Total params: 3,517,167\n",
      "Trainable params: 3,517,167\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKY5u7VMlRTX"
   },
   "source": [
    "### Make a custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1623203860105,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "PFyIDYZllb_V"
   },
   "outputs": [],
   "source": [
    "# define loss objective\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 516,
     "status": "ok",
     "timestamp": 1623203860606,
     "user": {
      "displayName": "Raymond Ngiam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjyeB8nVwl4aahdaHXzAfTJ5tfTEuviw8xXhOsl=s64",
      "userId": "11132753723286663537"
     },
     "user_tz": -480
    },
    "id": "HZJoqq8RlnR8"
   },
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01,momentum=0.9,nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom model via model subclassing \n",
    "class NeuralTranslationModel(Model):\n",
    "    def __init__(self,encoder_input_shape,decoder_input_shape):\n",
    "        super(NeuralTranslationModel, self).__init__()\n",
    "        self.encoder = Encoder(input_shape=encoder_input_shape)\n",
    "        self.decoder = Decoder(input_embedding_dim=decoder_input_shape)\n",
    "        self.model_trainable_variables = self.encoder.trainable_variables + \\\n",
    "                                         self.decoder.trainable_variables    \n",
    "  \n",
    "    def chinese_data_io(self,chinese_data):\n",
    "        input_data = chinese_data[:,0:tf.shape(chinese_data)[1]-1]\n",
    "        output_data = chinese_data[:,1:tf.shape(chinese_data)[1]]\n",
    "        return(input_data,output_data)\n",
    "\n",
    "    def call(self,inputs):\n",
    "        (encoder_in, decoder_in)=inputs\n",
    "        hidden_state ,cell_state = self.encoder(encoder_in)\n",
    "        dense_output, _, _ = self.decoder(decoder_in, hidden_state, cell_state)\n",
    "        return dense_output\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self,data):        \n",
    "        (english,chinese) = data\n",
    "        chinese_input, chinese_output = self.chinese_data_io(chinese)  \n",
    "        with tf.GradientTape() as tape:        \n",
    "            hidden_state ,cell_state = self.encoder(english)\n",
    "            dense_output, _, _ = self.decoder(chinese_input, hidden_state, cell_state)\n",
    "            loss = tf.math.reduce_mean(self.compiled_loss(chinese_output,dense_output))\n",
    "            grads = tape.gradient(loss, self.model_trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grads,\n",
    "                                               self.model_trainable_variables))\n",
    "            self.compiled_metrics.update_state(chinese_output,dense_output)\n",
    "        return {m.name:m.result() for m in self.metrics}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        (english,chinese) = data\n",
    "        chinese_input, chinese_output = self.chinese_data_io(chinese) \n",
    "        hidden_state ,cell_state = self.encoder(english)\n",
    "        dense_output, _, _ = self.decoder(chinese_input, hidden_state, cell_state)\n",
    "        loss = tf.math.reduce_mean(self.compiled_loss(chinese_output,dense_output))\n",
    "        self.compiled_metrics.update_state(chinese_output,dense_output)\n",
    "        return {m.name:m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 46, 128])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_in=tf.zeros([1,max_len_in_chinese_tokenized,128])\n",
    "encoder_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_in=tf.Variable([[1]])\n",
    "decoder_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# instantiate and compile the custom model\n",
    "translation_model = NeuralTranslationModel(encoder_input_shape=(max_len_in_chinese_tokenized,128),\n",
    "                                           decoder_input_shape=(max_word_index + 1, 128))\n",
    "\n",
    "# build the model by calling it\n",
    "translation_model((encoder_in,decoder_in))\n",
    "\n",
    "translation_model.compile(optimizer = optimizer,\n",
    "                          loss = loss_object,\n",
    "                          metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1205/1205 [==============================] - 237s 191ms/step - loss: 2.1232 - sparse_categorical_accuracy: 0.0375 - val_loss: 2.1167 - val_sparse_categorical_accuracy: 0.0454\n",
      "\n",
      "Epoch 00001: val_sparse_categorical_accuracy improved from -inf to 0.04537, saving model to models/eng-to-ch/checkpoint_best/checkpoint\n",
      "Epoch 2/2\n",
      "1205/1205 [==============================] - 227s 189ms/step - loss: 2.1193 - sparse_categorical_accuracy: 0.0431 - val_loss: 2.1098 - val_sparse_categorical_accuracy: 0.0412\n",
      "\n",
      "Epoch 00002: val_sparse_categorical_accuracy did not improve from 0.04537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f194eb1e070>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define callbacks\n",
    "checkpoint_epoch = ModelCheckpoint(filepath='models/eng-to-ch/checkpoint_epoch/checkpoint_{epoch}',\n",
    "                                   save_weights_only=True,\n",
    "                                   save_freq='epoch',\n",
    "                                   verbose=1)\n",
    "checkpoint_best = ModelCheckpoint(filepath='models/eng-to-ch/checkpoint_best/checkpoint',\n",
    "                                   save_weights_only=True,\n",
    "                                   save_freq='epoch',\n",
    "                                   save_best_only=True,\n",
    "                                   monitor='val_sparse_categorical_accuracy',\n",
    "                                   verbose=1)\n",
    "lr_reduce_plateau = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                      factor=0.05, \n",
    "                                      patience=5, \n",
    "                                      verbose=1, \n",
    "                                      mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               min_delta=0, \n",
    "                               patience=10, \n",
    "                               verbose=1,\n",
    "                               mode='min')\n",
    "\n",
    "callbacks=[checkpoint_best,lr_reduce_plateau,early_stopping]\n",
    "\n",
    "# fit the model\n",
    "steps_per_epoch = ceil(len(x_train)/int(batch_size))\n",
    "validation_steps = ceil(len(x_test)/int(batch_size))\n",
    "translation_model.fit(training_dataset_final,\n",
    "                      epochs=2,\n",
    "                      steps_per_epoch=steps_per_epoch,\n",
    "                      validation_data=validation_dataset_final,\n",
    "                      validation_steps=validation_steps,\n",
    "                      callbacks=callbacks\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder and decoder models can be accessed through the custom model as followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7f191340cc10>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Decoder at 0x7f1913200250>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_model.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making translation with the custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = word_index['<start>']\n",
    "end_token = word_index['<end>']\n",
    "inv_chinese_index = {value:key for key,value in tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(english_split_in):\n",
    "    eng_embedding = embedding_layer(english_split_in)\n",
    "    eng_padded = tf.pad(eng_embedding, \n",
    "                        [[max_len_in_chinese_tokenized-len(eng_embedding), 0], \n",
    "                         [0, 0]], \n",
    "                        constant_values = 0)\n",
    "    english_expand = tf.expand_dims(eng_padded, 0)\n",
    "    hidden_state, cell_state = translation_model.encoder(english_expand)\n",
    "\n",
    "    current_translation = []\n",
    "    current_token = tf.Variable([[start_token]])\n",
    "\n",
    "    while (len(current_translation) <= max_len_in_chinese_tokenized):\n",
    "        out1, hidden_state, cell_state = translation_model.decoder(current_token,hidden_state,cell_state)\n",
    "        out2 = tf.argmax(out1, axis=2).numpy()[0,0]\n",
    "        current_token = tf.Variable([[out2]])\n",
    "        if out2 == end_token:\n",
    "            break\n",
    "        else:\n",
    "            current_translation.append(out2)\n",
    "    inv_tokenized = [inv_chinese_index[w] for w in current_translation]\n",
    "    inv_tokenized_string = ' '.join(inv_tokenized)\n",
    "    return inv_tokenized_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hurry', 'up', '.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['english_split'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我 我 我 我 我 我 我 我 我 我 我 坏 鬆 鬆 赖 赖 赖 棒 辣 辣 裕 匈 辣 守 匈 憶 憶 砸 挣 壽 猛 榜 滋 绳 貓 遮 遮 跃 墳 敗 尤 洛 洛 薪 薪 助 苗'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(df['english_split'][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, the model has not been fully trained. However, we have verify the functionality of the entire pipeline.\n",
    "\n",
    "Up next, we will train the model in the next notebook."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPvBD2oewcRjou2z92vLHnP",
   "collapsed_sections": [],
   "name": "03-TranslationalModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
